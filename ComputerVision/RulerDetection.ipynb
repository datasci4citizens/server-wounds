{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics opencv-python matplotlib pyyaml shapely\n",
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GzCx-55lAhyt",
        "outputId": "576a1dc7-5c6d-4c0f-df17-66cb17be158a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.151)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (2.1.1)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "from shapely.geometry import Polygon\n",
        "import matplotlib.patches as patches"
      ],
      "metadata": {
        "id": "kibp8Q84IbyI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RulerDetector:\n",
        "    def __init__(self, model_path=None):\n",
        "        \"\"\"\n",
        "        Inicializa o detector de régua para seu dataset específico\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Caminho para o modelo treinado. Se None, usa modelo base YOLOv11\n",
        "        \"\"\"\n",
        "        if model_path and os.path.exists(model_path):\n",
        "            self.model = YOLO(model_path)\n",
        "            print(f\"✅ Modelo carregado de: {model_path}\")\n",
        "        else:\n",
        "            # Usa modelo de segmentação para aproveitar coordenadas de polígono\n",
        "            self.model = YOLO('yolo11n-seg.pt')\n",
        "            print(\"🔄 Modelo base YOLOv11n-seg carregado para treinamento\")\n",
        "\n",
        "        # Configurações específicas do seu dataset\n",
        "        self.class_names = ['0', 'Penggaris', 'Penghapus', 'Pulpen']\n",
        "        self.ruler_class_id = 1  # 'Penggaris' é a classe 1 (régua)\n",
        "\n",
        "        print(f\"🎯 Classes do dataset: {self.class_names}\")\n",
        "        print(f\"📏 Classe da régua: {self.class_names[self.ruler_class_id]} (ID: {self.ruler_class_id})\")\n",
        "\n",
        "    def verify_dataset_structure(self, dataset_path):\n",
        "        \"\"\"\n",
        "        Verifica e exibe a estrutura do dataset\n",
        "        \"\"\"\n",
        "        dataset_path = Path(dataset_path)\n",
        "        print(f\"\\n📁 Verificando estrutura do dataset em: {dataset_path}\")\n",
        "\n",
        "        # Verifica pastas principais\n",
        "        folders_found = {}\n",
        "        for folder in ['train', 'valid', 'test']:\n",
        "            folder_path = dataset_path / folder\n",
        "            if folder_path.exists():\n",
        "                images_path = folder_path / 'images'\n",
        "                labels_path = folder_path / 'labels'\n",
        "\n",
        "                img_count = len(list(images_path.glob('*'))) if images_path.exists() else 0\n",
        "                lbl_count = len(list(labels_path.glob('*.txt'))) if labels_path.exists() else 0\n",
        "\n",
        "                folders_found[folder] = {\n",
        "                    'images': img_count,\n",
        "                    'labels': lbl_count,\n",
        "                    'images_exist': images_path.exists(),\n",
        "                    'labels_exist': labels_path.exists()\n",
        "                }\n",
        "\n",
        "                print(f\"  📂 {folder}/: {img_count} imagens, {lbl_count} labels\")\n",
        "\n",
        "        return folders_found\n",
        "\n",
        "    def analyze_annotations(self, dataset_path, sample_size=5):\n",
        "        \"\"\"\n",
        "        Analisa anotações para entender o formato\n",
        "        \"\"\"\n",
        "        dataset_path = Path(dataset_path)\n",
        "        print(f\"\\n🔍 Analisando formato das anotações...\")\n",
        "\n",
        "        # Procura arquivos de anotação\n",
        "        label_files = []\n",
        "        for folder in ['train', 'valid']:\n",
        "            labels_path = dataset_path / folder / 'labels'\n",
        "            if labels_path.exists():\n",
        "                label_files.extend(list(labels_path.glob('*.txt'))[:sample_size])\n",
        "\n",
        "        if not label_files:\n",
        "            print(\"❌ Nenhum arquivo de anotação encontrado!\")\n",
        "            return\n",
        "\n",
        "        annotation_info = {\n",
        "            'total_files_analyzed': 0,\n",
        "            'classes_found': set(),\n",
        "            'ruler_annotations': 0,\n",
        "            'sample_annotations': []\n",
        "        }\n",
        "\n",
        "        for label_file in label_files[:sample_size]:\n",
        "            if not label_file.exists():\n",
        "                continue\n",
        "\n",
        "            annotation_info['total_files_analyzed'] += 1\n",
        "\n",
        "            with open(label_file, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "            for line in lines:\n",
        "                if line.strip():\n",
        "                    parts = list(map(float, line.strip().split()))\n",
        "                    class_id = int(parts[0])\n",
        "                    coords = parts[1:]\n",
        "\n",
        "                    annotation_info['classes_found'].add(class_id)\n",
        "\n",
        "                    if class_id == self.ruler_class_id:\n",
        "                        annotation_info['ruler_annotations'] += 1\n",
        "\n",
        "                        if len(annotation_info['sample_annotations']) < 2:\n",
        "                            annotation_info['sample_annotations'].append({\n",
        "                                'file': label_file.name,\n",
        "                                'class_id': class_id,\n",
        "                                'num_points': len(coords) // 2,\n",
        "                                'coords_sample': coords[:8]  # Primeiros 4 pontos\n",
        "                            })\n",
        "\n",
        "        # Exibe resultados da análise\n",
        "        print(f\"📊 Arquivos analisados: {annotation_info['total_files_analyzed']}\")\n",
        "        print(f\"🏷️ Classes encontradas: {sorted(annotation_info['classes_found'])}\")\n",
        "        print(f\"📏 Anotações de régua encontradas: {annotation_info['ruler_annotations']}\")\n",
        "\n",
        "        for sample in annotation_info['sample_annotations']:\n",
        "            print(f\"  📝 {sample['file']}: Classe {sample['class_id']}, {sample['num_points']} pontos\")\n",
        "\n",
        "        return annotation_info\n",
        "\n",
        "    def setup_dataset(self, dataset_path):\n",
        "        \"\"\"\n",
        "        Configura o dataset para treinamento\n",
        "        \"\"\"\n",
        "        self.dataset_path = Path(dataset_path)\n",
        "\n",
        "        # Verifica estrutura\n",
        "        self.verify_dataset_structure(dataset_path)\n",
        "\n",
        "        # Analisa anotações\n",
        "        self.analyze_annotations(dataset_path)\n",
        "\n",
        "        # Verifica/cria data.yaml\n",
        "        config_path = self.dataset_path / \"data.yaml\"\n",
        "        if config_path.exists():\n",
        "            print(f\"✅ Arquivo data.yaml encontrado\")\n",
        "            # Verifica se está correto\n",
        "            with open(config_path, 'r') as f:\n",
        "                config = yaml.safe_load(f)\n",
        "            print(f\"   - Classes: {config.get('names', 'não encontradas')}\")\n",
        "        else:\n",
        "            print(\"📝 Criando arquivo data.yaml...\")\n",
        "            self.create_dataset_config(config_path)\n",
        "\n",
        "        return str(config_path)\n",
        "\n",
        "    def create_dataset_config(self, config_path):\n",
        "        \"\"\"\n",
        "        Cria arquivo de configuração do dataset baseado na sua estrutura\n",
        "        \"\"\"\n",
        "        config = {\n",
        "            'path': str(self.dataset_path.absolute()),\n",
        "            'train': 'train/images',\n",
        "            'val': 'valid/images',\n",
        "            'test': 'test/images',\n",
        "            'nc': 4,\n",
        "            'names': ['0', 'Penggaris', 'Penghapus', 'Pulpen']\n",
        "        }\n",
        "\n",
        "        with open(config_path, 'w') as f:\n",
        "            yaml.dump(config, f, default_flow_style=False)\n",
        "\n",
        "        print(f\"✅ Arquivo data.yaml criado: {config_path}\")\n",
        "\n",
        "    def train_model(self, dataset_path, epochs=100, img_size=640, batch_size=8, patience=20):\n",
        "        \"\"\"\n",
        "        Treina o modelo de segmentação\n",
        "        \"\"\"\n",
        "        print(\"🚀 Iniciando treinamento do modelo de segmentação...\")\n",
        "\n",
        "        # Configura dataset\n",
        "        config_file = self.setup_dataset(dataset_path)\n",
        "\n",
        "        try:\n",
        "            # Treina o modelo\n",
        "            results = self.model.train(\n",
        "                data=config_file,\n",
        "                epochs=epochs,\n",
        "                imgsz=img_size,\n",
        "                batch=batch_size,\n",
        "                name='ruler_segmentation',\n",
        "                save=True,\n",
        "                plots=True,\n",
        "                patience=patience,\n",
        "                device='cpu'  # Usa GPU se disponível\n",
        "            )\n",
        "\n",
        "            model_path = \"runs/segment/ruler_segmentation/weights/best.pt\"\n",
        "            print(f\"✅ Treinamento concluído!\")\n",
        "            print(f\"📁 Modelo salvo em: {model_path}\")\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Erro durante treinamento: {e}\")\n",
        "            print(\"💡 Dicas para resolver:\")\n",
        "            print(\"  - Verifique se as imagens e labels estão nas pastas corretas\")\n",
        "            print(\"  - Reduza o batch_size se houver erro de memória\")\n",
        "            print(\"  - Verifique se o dataset está no formato correto\")\n",
        "            raise\n",
        "\n",
        "    def load_trained_model(self, model_path):\n",
        "        \"\"\"\n",
        "        Carrega modelo já treinado\n",
        "        \"\"\"\n",
        "        if not os.path.exists(model_path):\n",
        "            print(f\"❌ Modelo não encontrado: {model_path}\")\n",
        "            return False\n",
        "\n",
        "        self.model = YOLO(model_path)\n",
        "        print(f\"✅ Modelo carregado: {model_path}\")\n",
        "        return True\n",
        "\n",
        "    def calculate_polygon_area_shoelace(self, points):\n",
        "        \"\"\"\n",
        "        Calcula área de polígono usando fórmula Shoelace\n",
        "        Mais precisa que usar apenas bounding box\n",
        "        \"\"\"\n",
        "        if len(points) < 6:  # Precisa de pelo menos 3 pontos (6 coordenadas)\n",
        "            return 0\n",
        "\n",
        "        # Organiza pontos em pares (x, y)\n",
        "        coords = []\n",
        "        for i in range(0, len(points), 2):\n",
        "            if i + 1 < len(points):\n",
        "                coords.append([points[i], points[i + 1]])\n",
        "\n",
        "        if len(coords) < 3:\n",
        "            return 0\n",
        "\n",
        "        # Fórmula Shoelace\n",
        "        area = 0\n",
        "        n = len(coords)\n",
        "        for i in range(n):\n",
        "            j = (i + 1) % n\n",
        "            area += coords[i][0] * coords[j][1]\n",
        "            area -= coords[j][0] * coords[i][1]\n",
        "\n",
        "        return abs(area) / 2\n",
        "\n",
        "    def detect_ruler(self, image_path, conf_threshold=0.3, save_crops=False):\n",
        "        \"\"\"\n",
        "        Detecta régua na imagem e calcula área real do polígono\n",
        "        \"\"\"\n",
        "        if not os.path.exists(image_path):\n",
        "            return {\"error\": f\"Imagem não encontrada: {image_path}\"}\n",
        "\n",
        "        print(f\"🔍 Analisando imagem: {Path(image_path).name}\")\n",
        "\n",
        "        # Faz predição\n",
        "        results = self.model(image_path, conf=conf_threshold, save_crop=save_crops)\n",
        "\n",
        "        # Carrega imagem para obter dimensões\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            return {\"error\": f\"Não foi possível carregar a imagem\"}\n",
        "\n",
        "        img_height, img_width = image.shape[:2]\n",
        "\n",
        "        detection_info = {\n",
        "            \"image_path\": image_path,\n",
        "            \"image_dimensions\": {\"width\": img_width, \"height\": img_height},\n",
        "            \"ruler_detected\": False,\n",
        "            \"ruler_detections\": [],\n",
        "            \"all_detections\": [],\n",
        "            \"total_ruler_area_pixels\": 0,\n",
        "            \"confidence_threshold\": conf_threshold\n",
        "        }\n",
        "\n",
        "        # Processa resultados\n",
        "        for result in results:\n",
        "            if result.boxes is not None:\n",
        "                for i, box in enumerate(result.boxes):\n",
        "                    class_id = int(box.cls[0].cpu().numpy())\n",
        "                    confidence = float(box.conf[0].cpu().numpy())\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "\n",
        "                    # Informações básicas da detecção\n",
        "                    detection = {\n",
        "                        \"class_id\": class_id,\n",
        "                        \"class_name\": self.class_names[class_id],\n",
        "                        \"confidence\": confidence,\n",
        "                        \"bbox\": [int(x1), int(y1), int(x2), int(y2)],\n",
        "                        \"bbox_area_pixels\": int((x2 - x1) * (y2 - y1))\n",
        "                    }\n",
        "\n",
        "                    # Tenta obter máscara de segmentação\n",
        "                    if (hasattr(result, 'masks') and\n",
        "                        result.masks is not None and\n",
        "                        i < len(result.masks.xy)):\n",
        "\n",
        "                        # Coordenadas da máscara (já em pixels)\n",
        "                        mask_coords = result.masks.xy[i]\n",
        "                        if len(mask_coords) > 0:\n",
        "                            # Converte para lista de coordenadas\n",
        "                            coords_flat = mask_coords.flatten().tolist()\n",
        "                            detection[\"segmentation_coords\"] = coords_flat\n",
        "                            detection[\"num_points\"] = len(coords_flat) // 2\n",
        "\n",
        "                            # Calcula área real do polígono\n",
        "                            polygon_area = self.calculate_polygon_area_shoelace(coords_flat)\n",
        "                            detection[\"polygon_area_pixels\"] = int(polygon_area)\n",
        "\n",
        "                            print(f\"  🎯 {detection['class_name']}: {detection['num_points']} pontos, área: {polygon_area:.0f} px\")\n",
        "\n",
        "                    detection_info[\"all_detections\"].append(detection)\n",
        "\n",
        "                    # Se é régua, adiciona às detecções de régua\n",
        "                    if class_id == self.ruler_class_id:\n",
        "                        detection_info[\"ruler_detected\"] = True\n",
        "                        detection_info[\"ruler_detections\"].append(detection)\n",
        "\n",
        "                        # Soma área (usa polígono se disponível, senão bbox)\n",
        "                        area = detection.get(\"polygon_area_pixels\", detection[\"bbox_area_pixels\"])\n",
        "                        detection_info[\"total_ruler_area_pixels\"] += area\n",
        "\n",
        "        # Resumo final\n",
        "        if detection_info[\"ruler_detected\"]:\n",
        "            print(f\"✅ {len(detection_info['ruler_detections'])} régua(s) detectada(s)\")\n",
        "            print(f\"📐 Área total: {detection_info['total_ruler_area_pixels']} pixels\")\n",
        "        else:\n",
        "            print(\"❌ Nenhuma régua detectada\")\n",
        "\n",
        "        print(f\"📊 Total de objetos: {len(detection_info['all_detections'])}\")\n",
        "\n",
        "        return detection_info\n",
        "\n",
        "    def visualize_detection(self, image_path, detection_info, save_path=None, show_all_objects=True):\n",
        "        \"\"\"\n",
        "        Visualiza resultados com polígonos precisos\n",
        "        \"\"\"\n",
        "        image = cv2.imread(image_path)\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(16, 12))\n",
        "        ax.imshow(image_rgb)\n",
        "\n",
        "        # Cores para cada classe\n",
        "        class_colors = {\n",
        "            0: 'blue',\n",
        "            1: 'red',      # Régua\n",
        "            2: 'green',    # Borracha\n",
        "            3: 'orange'    # Caneta\n",
        "        }\n",
        "\n",
        "        # Desenha todas as detecções\n",
        "        for detection in detection_info[\"all_detections\"]:\n",
        "            class_id = detection[\"class_id\"]\n",
        "            is_ruler = (class_id == self.ruler_class_id)\n",
        "            color = class_colors.get(class_id, 'purple')\n",
        "\n",
        "            # Desenha bounding box\n",
        "            x1, y1, x2, y2 = detection[\"bbox\"]\n",
        "            linewidth = 4 if is_ruler else 2\n",
        "            linestyle = '-' if is_ruler else '--'\n",
        "\n",
        "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1,\n",
        "                                   linewidth=linewidth, edgecolor=color,\n",
        "                                   facecolor='none', linestyle=linestyle)\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "            # Desenha polígono de segmentação se disponível\n",
        "            if \"segmentation_coords\" in detection:\n",
        "                coords = detection[\"segmentation_coords\"]\n",
        "                # Converte coordenadas planas para pontos (x,y)\n",
        "                points = []\n",
        "                for i in range(0, len(coords), 2):\n",
        "                    if i + 1 < len(coords):\n",
        "                        points.append([coords[i], coords[i + 1]])\n",
        "\n",
        "                if len(points) >= 3:\n",
        "                    polygon = MplPolygon(points, closed=True, fill=False,\n",
        "                                       edgecolor=color, linewidth=3, alpha=0.8)\n",
        "                    ax.add_patch(polygon)\n",
        "\n",
        "            # Informações do texto\n",
        "            confidence = detection[\"confidence\"]\n",
        "            bbox_area = detection[\"bbox_area_pixels\"]\n",
        "            polygon_area = detection.get(\"polygon_area_pixels\", None)\n",
        "\n",
        "            # Monta texto informativo\n",
        "            if is_ruler:\n",
        "                if polygon_area:\n",
        "                    text = f'🎯 RÉGUA\\nConf: {confidence:.2f}\\nÁrea real: {polygon_area:,} px\\nBBox: {bbox_area:,} px'\n",
        "                    text_color = 'white'\n",
        "                    bg_color = 'red'\n",
        "                    alpha = 0.9\n",
        "                else:\n",
        "                    text = f'📏 RÉGUA\\nConf: {confidence:.2f}\\nÁrea: {bbox_area:,} px'\n",
        "                    text_color = 'white'\n",
        "                    bg_color = 'red'\n",
        "                    alpha = 0.8\n",
        "            else:\n",
        "                if show_all_objects:\n",
        "                    area_text = f'{polygon_area:,}' if polygon_area else f'{bbox_area:,}'\n",
        "                    text = f'{detection[\"class_name\"]}\\n{confidence:.2f}\\n{area_text} px'\n",
        "                    text_color = 'white'\n",
        "                    bg_color = color\n",
        "                    alpha = 0.7\n",
        "                else:\n",
        "                    continue  # Pula objetos que não são régua\n",
        "\n",
        "            # Posiciona texto\n",
        "            text_y = y1 - 10 if y1 > 50 else y2 + 10\n",
        "            ax.text(x1, text_y, text,\n",
        "                   bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=bg_color, alpha=alpha),\n",
        "                   fontsize=11, color=text_color, weight='bold')\n",
        "\n",
        "        # Título da imagem\n",
        "        title = f\"📷 {Path(image_path).name}\\n\"\n",
        "\n",
        "        if detection_info[\"ruler_detected\"]:\n",
        "            ruler_count = len(detection_info[\"ruler_detections\"])\n",
        "            total_area = detection_info[\"total_ruler_area_pixels\"]\n",
        "            title += f\"✅ {ruler_count} Régua(s) Detectada(s) - Área Total: {total_area:,} pixels\"\n",
        "        else:\n",
        "            title += \"❌ Nenhuma Régua Detectada\"\n",
        "\n",
        "        total_objects = len(detection_info[\"all_detections\"])\n",
        "        title += f\"\\n📊 Total de Objetos: {total_objects}\"\n",
        "\n",
        "        ax.set_title(title, fontsize=14, weight='bold', pad=20)\n",
        "        ax.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, bbox_inches='tight', dpi=200)\n",
        "            print(f\"💾 Resultado salvo em: {save_path}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def batch_process(self, images_folder, output_folder=None):\n",
        "        \"\"\"\n",
        "        Processa múltiplas imagens\n",
        "        \"\"\"\n",
        "        images_folder = Path(images_folder)\n",
        "        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
        "\n",
        "        if output_folder:\n",
        "            output_folder = Path(output_folder)\n",
        "            output_folder.mkdir(exist_ok=True)\n",
        "\n",
        "        results = []\n",
        "        total_ruler_area = 0\n",
        "        images_with_rulers = 0\n",
        "\n",
        "        print(f\"\\n🔄 Processando imagens em: {images_folder}\")\n",
        "\n",
        "        image_files = [f for f in images_folder.iterdir()\n",
        "                      if f.suffix.lower() in image_extensions]\n",
        "\n",
        "        for i, img_file in enumerate(image_files, 1):\n",
        "            print(f\"\\n📸 [{i}/{len(image_files)}] {img_file.name}\")\n",
        "\n",
        "            # Detecta objetos\n",
        "            detection_result = self.detect_ruler(str(img_file))\n",
        "            detection_result[\"filename\"] = img_file.name\n",
        "            results.append(detection_result)\n",
        "\n",
        "            # Atualiza estatísticas\n",
        "            if detection_result[\"ruler_detected\"]:\n",
        "                images_with_rulers += 1\n",
        "                total_ruler_area += detection_result[\"total_ruler_area_pixels\"]\n",
        "\n",
        "            # Salva visualização se solicitado\n",
        "            if output_folder:\n",
        "                save_path = output_folder / f\"resultado_{img_file.name}\"\n",
        "                self.visualize_detection(str(img_file), detection_result, str(save_path))\n",
        "\n",
        "        # Resumo final\n",
        "        print(f\"\\n📊 === RESUMO DO PROCESSAMENTO ===\")\n",
        "        print(f\"🖼️ Imagens processadas: {len(image_files)}\")\n",
        "        print(f\"📏 Imagens com réguas: {images_with_rulers}\")\n",
        "        print(f\"📐 Área total de réguas: {total_ruler_area:,} pixels\")\n",
        "\n",
        "        if images_with_rulers > 0:\n",
        "            avg_area = total_ruler_area / images_with_rulers\n",
        "            print(f\"📊 Área média por imagem: {avg_area:.0f} pixels\")\n",
        "\n",
        "        return results\n"
      ],
      "metadata": {
        "id": "o66zhbScI0k4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"🎯 === DETECTOR DE RÉGUA - VERSÃO OTIMIZADA ===\")\n",
        "    print(\"📋 Adaptado especificamente para seu dataset com 4 classes\")\n",
        "\n",
        "    # Configuração\n",
        "    dataset_path = \"/content/drive/MyDrive/MC854/DeteksiPerlengkapanSeklah.v2i.yolov11\"  # Substitua pelo caminho real\n",
        "    test_image = \"/content/11T8.JPG\"  # Substitua pelo caminho real\n",
        "\n",
        "    print(f\"\\n🔧 EXEMPLO DE USO:\")\n",
        "    print(f\"1️⃣ TREINAMENTO:\")\n",
        "    print(f\"   detector = RulerDetector()\")\n",
        "    print(f\"   results = detector.train_model('{dataset_path}', epochs=50)\")\n",
        "\n",
        "    print(f\"\\n2️⃣ DETECÇÃO:\")\n",
        "    print(f\"   detector.load_trained_model('runs/segment/ruler_segmentation/weights/best.pt')\")\n",
        "    print(f\"   result = detector.detect_ruler('{test_image}')\")\n",
        "    print(f\"   detector.visualize_detection('{test_image}', result)\")\n",
        "\n",
        "    print(f\"\\n💡 CARACTERÍSTICAS:\")\n",
        "    print(f\"   ✅ Detecta todas as 4 classes do seu dataset\")\n",
        "    print(f\"   ✅ Calcula área REAL da régua (polígono, não retângulo)\")\n",
        "    print(f\"   ✅ Visualização com contornos precisos\")\n",
        "    print(f\"   ✅ Processamento em lote de múltiplas imagens\")\n",
        "\n",
        "    # Exemplo prático (descomente para usar)\n",
        "\n",
        "    # Inicializa detector\n",
        "    detector = RulerDetector()\n",
        "\n",
        "    # Treina modelo\n",
        "    results = detector.train_model(\n",
        "        dataset_path=dataset_path,\n",
        "        epochs=50,\n",
        "        batch_size=8,\n",
        "        patience=10\n",
        "    )\n",
        "\n",
        "    # Testa em imagem\n",
        "    detector.load_trained_model(\"runs/segment/ruler_segmentation/weights/best.pt\")\n",
        "    result = detector.detect_ruler(test_image)\n",
        "    detector.visualize_detection(test_image, result)\n",
        "\n",
        "    print(f\"Régua detectada: {result['ruler_detected']}\")\n",
        "    print(f\"Área da régua: {result['total_ruler_area_pixels']} pixels\")\n",
        ""
      ],
      "metadata": {
        "id": "8AnzhkczJOD9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print()\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9gn5zIhvJRhi",
        "outputId": "e613e410-3419-4423-daa3-f3d1412b80b0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎯 === DETECTOR DE RÉGUA - VERSÃO OTIMIZADA ===\n",
            "📋 Adaptado especificamente para seu dataset com 4 classes\n",
            "\n",
            "🔧 EXEMPLO DE USO:\n",
            "1️⃣ TREINAMENTO:\n",
            "   detector = RulerDetector()\n",
            "   results = detector.train_model('/content/drive/MyDrive/MC854/DeteksiPerlengkapanSeklah.v2i.yolov11', epochs=50)\n",
            "\n",
            "2️⃣ DETECÇÃO:\n",
            "   detector.load_trained_model('runs/segment/ruler_segmentation/weights/best.pt')\n",
            "   result = detector.detect_ruler('/content/11T8.JPG')\n",
            "   detector.visualize_detection('/content/11T8.JPG', result)\n",
            "\n",
            "💡 CARACTERÍSTICAS:\n",
            "   ✅ Detecta todas as 4 classes do seu dataset\n",
            "   ✅ Calcula área REAL da régua (polígono, não retângulo)\n",
            "   ✅ Visualização com contornos precisos\n",
            "   ✅ Processamento em lote de múltiplas imagens\n",
            "🔄 Modelo base YOLOv11n-seg carregado para treinamento\n",
            "🎯 Classes do dataset: ['0', 'Penggaris', 'Penghapus', 'Pulpen']\n",
            "📏 Classe da régua: Penggaris (ID: 1)\n",
            "🚀 Iniciando treinamento do modelo de segmentação...\n",
            "\n",
            "📁 Verificando estrutura do dataset em: /content/drive/MyDrive/MC854/DeteksiPerlengkapanSeklah.v2i.yolov11\n",
            "  📂 train/: 313 imagens, 313 labels\n",
            "  📂 valid/: 90 imagens, 90 labels\n",
            "  📂 test/: 43 imagens, 43 labels\n",
            "\n",
            "🔍 Analisando formato das anotações...\n",
            "📊 Arquivos analisados: 5\n",
            "🏷️ Classes encontradas: [1, 2]\n",
            "📏 Anotações de régua encontradas: 3\n",
            "  📝 ruler_190_jpg.rf.f0460cde13e80f09582e722b4f76314b.txt: Classe 1, 6 pontos\n",
            "  📝 5c114cf3a4db3438f0e520b0_png_jpg.rf.60667e0e18d0217db67dbd77609397a5.txt: Classe 1, 62 pontos\n",
            "✅ Arquivo data.yaml encontrado\n",
            "   - Classes: ['0', 'Penggaris', 'Penghapus', 'Pulpen']\n",
            "Ultralytics 8.3.151 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (AMD EPYC 7B12)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/drive/MyDrive/MC854/DeteksiPerlengkapanSeklah.v2i.yolov11/data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11n-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=ruler_segmentation3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/segment/ruler_segmentation3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
            "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
            "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
            " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
            " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
            " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
            " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
            " 23        [16, 19, 22]  1    684220  ultralytics.nn.modules.head.Segment          [4, 32, 64, [64, 128, 256]]   \n",
            "YOLO11n-seg summary: 203 layers, 2,843,388 parameters, 2,843,372 gradients, 10.4 GFLOPs\n",
            "\n",
            "Transferred 510/561 items from pretrained weights\n",
            "Freezing layer 'model.23.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.8±0.3 ms, read: 15.9±10.1 MB/s, size: 31.1 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/MC854/DeteksiPerlengkapanSeklah.v2i.yolov11/train/labels.cache... 313 images, 0 backgrounds, 0 corrupt: 100%|██████████| 313/313 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 340, len(boxes) = 444. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.4±0.2 ms, read: 12.3±4.9 MB/s, size: 22.8 KB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/MC854/DeteksiPerlengkapanSeklah.v2i.yolov11/valid/labels.cache... 90 images, 0 backgrounds, 0 corrupt: 100%|██████████| 90/90 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 79, len(boxes) = 116. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting labels to runs/segment/ruler_segmentation3/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 90 weight(decay=0.0), 101 weight(decay=0.0005), 100 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/segment/ruler_segmentation3\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/40 [00:09<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Erro durante treinamento: ERROR ❌ segment dataset incorrectly formatted or not a segment dataset.\n",
            "This error can occur when incorrectly training a 'segment' model on a 'detect' dataset, i.e. 'yolo train model=yolo11n-seg.pt data=coco8.yaml'.\n",
            "Verify your dataset is a correctly formatted 'segment' dataset using 'data=coco8-seg.yaml' as an example.\n",
            "See https://docs.ultralytics.com/datasets/segment/ for help.\n",
            "💡 Dicas para resolver:\n",
            "  - Verifique se as imagens e labels estão nas pastas corretas\n",
            "  - Reduza o batch_size se houver erro de memória\n",
            "  - Verifique se o dataset está no formato correto\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ERROR ❌ segment dataset incorrectly formatted or not a segment dataset.\nThis error can occur when incorrectly training a 'segment' model on a 'detect' dataset, i.e. 'yolo train model=yolo11n-seg.pt data=coco8.yaml'.\nVerify your dataset is a correctly formatted 'segment' dataset using 'data=coco8-seg.yaml' as an example.\nSee https://docs.ultralytics.com/datasets/segment/ for help.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/loss.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mbatch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_idx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cls\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bboxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimgsz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 17 but got size 0 for tensor number 1 in the list.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e81c4ca57652>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-d12c71dd96f2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Treina modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     results = detector.train_model(\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mdataset_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-bddf5837655e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, dataset_path, epochs, img_size, batch_size, patience)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# Treina o modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             results = self.model.train(\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m  \u001b[0;31m# attach optional HUB session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \"\"\"\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# for cases of training and validating while training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"img\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/utils/loss.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mmask_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_bboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"ERROR ❌ segment dataset incorrectly formatted or not a segment dataset.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0;34m\"This error can occur when incorrectly training a 'segment' model on a 'detect' dataset, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ERROR ❌ segment dataset incorrectly formatted or not a segment dataset.\nThis error can occur when incorrectly training a 'segment' model on a 'detect' dataset, i.e. 'yolo train model=yolo11n-seg.pt data=coco8.yaml'.\nVerify your dataset is a correctly formatted 'segment' dataset using 'data=coco8-seg.yaml' as an example.\nSee https://docs.ultralytics.com/datasets/segment/ for help."
          ]
        }
      ]
    }
  ]
}